# Mechanistic Interpretability - Paper List

## Foundational Work
* **A Mathematical Framework for Transformer Circuits** (Nelson Elhage et al, Anthropic)

## Superposition
* **Toy models of superposition** (Nelson Elhage et al, Anthropic)
* Finding Neurons In A Haystack (Wes Gurnee et al)
* Fact Finding (Neel Nanda & Sen Rajamanoharan et al, Google DeepMind)

## Sparse Autoencoders (SAE)
* **Towards Monosemanticity** (Trenton Bricken et al, Anthropic)
* Sparse Feature Circuits (Sam Marks et al)
* Transcoders Find Interpretable LLM Feature Circuits (Jacob Dunefsky, Philippe Chlenski et al)
* Interpreting Attention Layer Outputs with Sparse Autoencoders (Connor Kissane & Rob Krzyzanowski et al)
* Towards principled evaluations of sparse autoencoders for interpretability and control (Alex Makelov & Georg Lange et al)
* Gated SAEs (Sen Rajamanoharan et al, DeepMind)
* Scaling and evaluating sparse autoencoders (Leo Gao et al, OpenAI)
* Scaling monosemanticity (Adly Templeton et al, Anthropic)

## Activation Patching
* How to use and interpret activation patching (Stefan Heimersheim & Neel Nanda)
* Causal scrubbing (Redwood)
* Attribution Patching (Neel Nanda, Anthropic)
* AtP* (Janos Kramar et al, Google DeepMind)
* Automated Circuit Discovery (Arthur Conmy et al)
* Distributed Alignment Search (Atticus Geiger et al)
* An Interpretability Illusion for Subspace Activation Patching (Aleksander Makelov & Georg Lange)

## Narrow Circuits
* **Indirect Object Identification** (Kevin Wang et al, Redwood)
* A Greater-Than Circuit (Michael Hanna et al, Redwood)
* Does Circuit Analysis Interpretability Scale? (Tom Lieberum et al, DeepMind)
